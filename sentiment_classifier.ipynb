{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd007efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Create DataFrames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from random import shuffle\n",
    "from os import system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = 'csv/annotation01_tsukuba_corpus_20140930.tsv'\n",
    "\n",
    "reader = pd.read_csv(csv_dir, sep='\\t', usecols=[4, 5], header=None)\n",
    "#reader.sample(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_classify_sentence(sentimentList: list, csvTable: pd.DataFrame, labelName: str) -> list:\n",
    "    for idx,data in csvTable.iterrows():\n",
    "        if (data[4] == labelName):\n",
    "            sentimentList.append((data[5], data[4]))\n",
    "\n",
    "dataList = []\n",
    "append_classify_sentence(dataList, reader, 'p')\n",
    "append_classify_sentence(dataList, reader, 'k')\n",
    "append_classify_sentence(dataList, reader, 'y')\n",
    "append_classify_sentence(dataList, reader, 'e')\n",
    "shuffle(dataList) # 3157 records in total\n",
    "\n",
    "train_list = dataList[:2657]\n",
    "test_list = dataList[-500:]\n",
    "\n",
    "def create_data_frame(tlist: list) -> pd.DataFrame:\n",
    "    text, label = tuple(zip(*tlist))\n",
    "    text = list(map(lambda txt: re.sub('(<br\\s*/?>)+', ' ', txt), text)) # replacing line breaks with spaces\n",
    "\n",
    "    return pd.DataFrame({'text': text, 'label': label})\n",
    "\n",
    "tsukuba_train = create_data_frame(train_list)\n",
    "tsukuba_test = create_data_frame(test_list)\n",
    "# tsukuba_train\n",
    "# tsukuba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text label\n",
       "0                   子連れだと、気になること等色々あるのですが、みんな楽しく過ごせました。     p\n",
       "1     ちょっと接客慣れしていない感じの方が２人ほど対応してくださっていましたが、他にチェックインな...     k\n",
       "2                                          お得なプランで満足です。     p\n",
       "3     また隣接するショッピングモールで買い物をすると観覧車が無料になるとネットで見たのであの広いモ...     k\n",
       "4           バスルームにも違いがあり、今回は浴槽の水深が深いタイプで、肩までどっぷり漬かれました。     p\n",
       "...                                                 ...   ...\n",
       "2652  ちなみにチェックアウトは余りにもスムーズで“ウェルカム”ってよりは“ウェルリーヴ”ってのがこ...     k\n",
       "2653          希望の部屋もこちらの無理を言ってお願いしましたが用意してくれてあって良かったです。     p\n",
       "2654       一人で泊まれる温泉旅館のプランは他にはあまりないので、このプランはとてもありがたいです。     p\n",
       "2655                               駅からは遠いが繁華街が近いのはメリット。     e\n",
       "2656                                      コンビニも近くてよかった。     p\n",
       "\n",
       "[2657 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>子連れだと、気になること等色々あるのですが、みんな楽しく過ごせました。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ちょっと接客慣れしていない感じの方が２人ほど対応してくださっていましたが、他にチェックインな...</td>\n      <td>k</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>お得なプランで満足です。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>また隣接するショッピングモールで買い物をすると観覧車が無料になるとネットで見たのであの広いモ...</td>\n      <td>k</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>バスルームにも違いがあり、今回は浴槽の水深が深いタイプで、肩までどっぷり漬かれました。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2652</th>\n      <td>ちなみにチェックアウトは余りにもスムーズで“ウェルカム”ってよりは“ウェルリーヴ”ってのがこ...</td>\n      <td>k</td>\n    </tr>\n    <tr>\n      <th>2653</th>\n      <td>希望の部屋もこちらの無理を言ってお願いしましたが用意してくれてあって良かったです。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>2654</th>\n      <td>一人で泊まれる温泉旅館のプランは他にはあまりないので、このプランはとてもありがたいです。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>2655</th>\n      <td>駅からは遠いが繁華街が近いのはメリット。</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>2656</th>\n      <td>コンビニも近くてよかった。</td>\n      <td>p</td>\n    </tr>\n  </tbody>\n</table>\n<p>2657 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "tsukuba_train.to_csv('csv/tsukuba_train.csv', index=False)\n",
    "tsukuba_test.to_csv('csv/tsukuba_test.csv', index=False)"
   ]
  },
  {
   "source": [
    "# Text Vectorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text label\n",
       "0                   子連れだと、気になること等色々あるのですが、みんな楽しく過ごせました。     p\n",
       "1     ちょっと接客慣れしていない感じの方が２人ほど対応してくださっていましたが、他にチェックインな...     k\n",
       "2                                          お得なプランで満足です。     p\n",
       "3     また隣接するショッピングモールで買い物をすると観覧車が無料になるとネットで見たのであの広いモ...     k\n",
       "4           バスルームにも違いがあり、今回は浴槽の水深が深いタイプで、肩までどっぷり漬かれました。     p\n",
       "...                                                 ...   ...\n",
       "2652  ちなみにチェックアウトは余りにもスムーズで“ウェルカム”ってよりは“ウェルリーヴ”ってのがこ...     k\n",
       "2653          希望の部屋もこちらの無理を言ってお願いしましたが用意してくれてあって良かったです。     p\n",
       "2654       一人で泊まれる温泉旅館のプランは他にはあまりないので、このプランはとてもありがたいです。     p\n",
       "2655                               駅からは遠いが繁華街が近いのはメリット。     e\n",
       "2656                                      コンビニも近くてよかった。     p\n",
       "\n",
       "[2657 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>子連れだと、気になること等色々あるのですが、みんな楽しく過ごせました。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ちょっと接客慣れしていない感じの方が２人ほど対応してくださっていましたが、他にチェックインな...</td>\n      <td>k</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>お得なプランで満足です。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>また隣接するショッピングモールで買い物をすると観覧車が無料になるとネットで見たのであの広いモ...</td>\n      <td>k</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>バスルームにも違いがあり、今回は浴槽の水深が深いタイプで、肩までどっぷり漬かれました。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2652</th>\n      <td>ちなみにチェックアウトは余りにもスムーズで“ウェルカム”ってよりは“ウェルリーヴ”ってのがこ...</td>\n      <td>k</td>\n    </tr>\n    <tr>\n      <th>2653</th>\n      <td>希望の部屋もこちらの無理を言ってお願いしましたが用意してくれてあって良かったです。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>2654</th>\n      <td>一人で泊まれる温泉旅館のプランは他にはあまりないので、このプランはとてもありがたいです。</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>2655</th>\n      <td>駅からは遠いが繁華街が近いのはメリット。</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>2656</th>\n      <td>コンビニも近くてよかった。</td>\n      <td>p</td>\n    </tr>\n  </tbody>\n</table>\n<p>2657 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n",
    "import nagisa # A Japanese tokenizer to ensure CountVectorizer can be executed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "## define a japanese tokenizer\n",
    "def tokenize_jp(doc):\n",
    "    doc = nagisa.filter(doc, filter_postags=['助詞', '補助記号', '助動詞'])\n",
    "    return doc.words\n",
    "\n",
    "## Unigram Counts\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1,1), tokenizer=tokenize_jp)\n",
    "unigram_vectorizer.fit_transform(tsukuba_train['text'].values)\n",
    "\n",
    "dump(unigram_vectorizer, 'data_preprocessors/unigram_vectorizer.joblib')\n",
    "# unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')\n",
    "\n",
    "X_train_unigram = unigram_vectorizer.transform(tsukuba_train['text'].values)\n",
    "\n",
    "save_npz('vectorized_data/X_train_unigram.npz', X_train_unigram)\n",
    "# X_train_unigram = load_npz('vectorized_data/X_train_unigram.npz')\n",
    "\n",
    "## Unigram Tf-Idf\n",
    "unigram_tf_idf_transformer = TfidfTransformer()\n",
    "unigram_tf_idf_transformer.fit(X_train_unigram)\n",
    "\n",
    "dump(unigram_tf_idf_transformer, 'data_preprocessors/unigram_tf_idf_transformer.joblib')\n",
    "# unigram_tf_idf_transformer = load('data_preprocessors/unigram_tf_idf_transformer.joblib')\n",
    "\n",
    "X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)\n",
    "\n",
    "save_npz('vectorized_data/X_train_unigram_tf_idf.npz', X_train_unigram_tf_idf)\n",
    "# X_train_unigram_tf_idf = load_npz('vectorized_data/X_train_unigram_tf_idf.npz')\n",
    "\n",
    "## Bigram Counts\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenize_jp)\n",
    "bigram_vectorizer.fit(tsukuba_train['text'].values)\n",
    "\n",
    "dump(bigram_vectorizer, 'data_preprocessors/bigram_vectorizer.joblib')\n",
    "# bigram_vectorizer = load('data_preprocessors/bigram_vectorizer.joblib')\n",
    "\n",
    "X_train_bigram = bigram_vectorizer.transform(tsukuba_train['text'].values)\n",
    "\n",
    "save_npz('vectorized_data/X_train_bigram.npz', X_train_bigram)\n",
    "# X_train_bigram = load_npz('vectorized_data/X_train_bigram.npz')\n",
    "\n",
    "## Bigram Tf-Idf\n",
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "\n",
    "dump(bigram_tf_idf_transformer, 'data_preprocessors/bigram_tf_idf_transformer.joblib')\n",
    "# bigram_tf_idf_transformer = load('data_preprocessors/bigram_tf_idf_transformer.joblib')\n",
    "\n",
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "\n",
    "save_npz('vectorized_data/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)\n",
    "# X_train_bigram_tf_idf = load_npz('vectorized_data/X_train_bigram_tf_idf.npz')"
   ]
  },
  {
   "source": [
    "# Choosing Data Format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unigram Counts\nTrain score: 1.0 ; Validation score: 0.74\n\nUnigram Tf-Idf\nTrain score: 1.0 ; Validation score: 0.75\n\nBigram Counts\nTrain score: 1.0 ; Validation score: 0.75\n\nBigram Tf-Idf\nTrain score: 1.0 ; Validation score: 0.77\n\n"
     ]
    }
   ],
   "source": [
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')\n",
    "\n",
    "y_train = tsukuba_train['label'].values\n",
    "\n",
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "source": [
    "# Using Cross-Validation for hyperparameter tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_bigram_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best params: {'eta0': 0.00995997003866562, 'learning_rate': 'adaptive', 'loss': 'squared_hinge'}\nBest score: 0.7715383090494599\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: loss, learning rate and initial learning rate\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    learning_rate=['optimal', 'invscaling', 'adaptive'],\n",
    "    eta0=uniform(loc=1e-7, scale=1e-2)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best params: {'alpha': 2.5534132650005583e-05, 'penalty': 'elasticnet'}\nBest score: 0.7749260156039817\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: penalty and alpha\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    penalty=['l1', 'l2', 'elasticnet'],\n",
    "    alpha=uniform(loc=1e-6, scale=1e-4)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "source": [
    "# Saving the best classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['classifiers/sgd_classifier.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "sgd_classifier = random_search_cv.best_estimator_\n",
    "\n",
    "dump(random_search_cv.best_estimator_, 'classifiers/sgd_classifier.joblib')\n",
    "# sgd_classifier = load('classifiers/sgd_classifier.joblib')"
   ]
  },
  {
   "source": [
    "# Testing model & scoring"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.772\n"
     ]
    }
   ],
   "source": [
    "X_test = bigram_vectorizer.transform(tsukuba_test['text'].values)\n",
    "X_test = bigram_tf_idf_transformer.transform(X_test)\n",
    "y_test = tsukuba_test['label'].values\n",
    "\n",
    "score = sgd_classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "source": [
    "# Using model to predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['p', 'k'], dtype='<U1')"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "model = load('classifiers/sgd_classifier.joblib')\n",
    "ozlab_predict = pd.read_csv('csv/ozlab_predict.csv', header=0)\n",
    "X_predict = bigram_vectorizer.transform(ozlab_predict['text'].values)\n",
    "X_predict = bigram_tf_idf_transformer.transform(X_predict)\n",
    "model.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}